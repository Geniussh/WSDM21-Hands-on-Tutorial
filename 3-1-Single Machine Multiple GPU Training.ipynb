{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complex-ready",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN with Multiple GPUs\n",
    "Note: this tutorial requires a GPU enabled machine with multiple gpu devices\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE on a single machine with multiple GPUs for node classification on ogbn-arxiv provided by Open Graph Benchmark (OGB). The dataset contains around 170 thousand nodes and 2 million edges.\n",
    "\n",
    "At the end of this tutorial you will be able to\n",
    "\n",
    " * Parallelize model training across multiple GPUs on a single device.\n",
    " * Distribute the model parameters using PyTorch DDP.\n",
    " \n",
    "## Distributed training overview\n",
    "Training models on very large datasets can take hours or even days to converge. In deep learning, we can get substantial speed-ups by distributing the training workload across multiple workers. Typically, workers run in parallel and can communicate their updates. Workers can be individual machines in a cluster (not covered in this tutorial). In this tutorial workers are processes in a single machine with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-sessions",
   "metadata": {},
   "source": [
    "### Data Parallelism\n",
    "\n",
    "For Multi-GPU training on a single machine, Data parallelism is an easy-to-implement and effective training approach.\n",
    "\n",
    "Here is how it works:\n",
    "\n",
    " * The data is divided into k partitions where k is the number of gpu workers.\n",
    " * The model is copied to each of the gpu workers.\n",
    " * Each worker operates on its own subset of the data.\n",
    " * Each worker communicates of its model changes to the other workers to update their corresponding model.\n",
    " \n",
    "PyTorch DistributedDataParallel (DDP) is the recommended built-in solution for multi-GPU training.\n",
    "\n",
    "You can use PyTorch DDP for DGL models in the same way for any other PyTorch applications.\n",
    "\n",
    " * Pytorch DDP implements data parallelism at the module level, therefore it wraps the model implementation.\n",
    " * To use it, your code needs to fork or spawn multiple processes, each with it's own DDP instance.\n",
    " * DDP uses collective communications to synchronize gradients and buffers.\n",
    " * For machines with Nvidia GPUs it's common using nccl as the communications backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-founder",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "OGB already prepared the data as DGL graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-grocery",
   "metadata": {},
   "source": [
    "OGB dataset is a collection of graphs and their labels. Ogbn-arxiv dataset only contains a single graph. So you can simply get the graph and its node labels like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "graph, node_labels = dataset[0]\n",
    "# Add reverse edges since ogbn-arxiv is unidirectional.\n",
    "graph = dgl.add_reverse_edges(graph)\n",
    "graph.ndata['label'] = node_labels[:, 0]\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-dream",
   "metadata": {},
   "source": [
    "You can get the training-validation-test split of the nodes with ``get_split_idx`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-academy",
   "metadata": {},
   "source": [
    "## Defining Neighbor Sampler and Data Loader\n",
    "We follow the previous **Training GNN with Neighbor Sampling for Node Classification** tutorial to define our data loader using ``dgl.dataloading.NodeDataLoader`` for iterating over the dataset and ``dgl.dataloading.MultiLayerNeighborSampler`` for randomly picking a fixed number of neighbors for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids):\n",
    "    partition_size = len(nids) // world_size  # split dataset into n GPUs\n",
    "    partition_offset = partition_size * rank\n",
    "    nids = nids[partition_offset:partition_offset+partition_size]\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-resort",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "The model implementation will be exactly the same as what you have seen in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class SageModel(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(SageModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()]\n",
    "        h = self.conv1(mfgs[0], (x, h_dst))\n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]\n",
    "        h = self.conv2(mfgs[1], (h, h_dst))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-literature",
   "metadata": {},
   "source": [
    "## Distributing the Model to GPUs\n",
    "PyTorch DDP manages the distribution of models and synchronization of the gradients for you.\n",
    "\n",
    "For DGL you can simply wrap the model with torch.nn.parallel.DistributedDataParallel.\n",
    "\n",
    "Here we make a simple function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def init_model(rank, in_feats, n_hidden, n_classes):\n",
    "    model = SageModel(in_feats, n_hidden, n_classes).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-petersburg",
   "metadata": {},
   "source": [
    "The recommended way to distribute training is to have one training process per GPU\n",
    "\n",
    "During model instantiation we also specify the process rank, which is equal to the GPU ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-bachelor",
   "metadata": {},
   "source": [
    "## The Training Loop for one Process\n",
    "The training loop for a single process running with a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fix_openmp as thread_wrapped_func\n",
    "import sklearn\n",
    "\n",
    "@thread_wrapped_func\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids)\n",
    "    \n",
    "    model = init_model(rank, num_features, 128, num_classes)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(train_dataloader):\n",
    "            mfgs = [mfg.to(rank) for mfg in mfgs]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = mfgs[-1].dstdata['label']\n",
    "            predictions = model(mfgs, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(),\n",
    "                                                      predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Train acc {:.04f} Loss {:.04f}'.format(\n",
    "                    epoch, step, accuracy, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        # GPU 0 will do the evaluation\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    inputs = node_features[input_nodes].cuda()\n",
    "                    labels.append(node_labels[output_nodes].numpy())\n",
    "                    predictions.append(model.module(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-possession",
   "metadata": {},
   "source": [
    "## Spawning multiple processes for the Multi GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = (graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes)\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-banana",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for node classification on a large dataset that cannot fit into GPU. The method you have learned can scale to a graph of any size, and works on a single machine with any number of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-transmission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
